#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-Cloud Space Synchronization Tool with Particle Globe Memory
å¤šé›²ç©ºé–“åŒæ­¥å·¥å…· - æ•´åˆç²’å­åœ°çƒå„€è¨˜æ†¶æ³•

åŒæ­¥å„å€‹é›²ç©ºé–“ï¼ˆåŒ…å«æ²™ç›’ç’°å¢ƒï¼‰ä¸¦é‹ç”¨ç²’å­åœ°çƒå„€è¨˜æ†¶æ³•é€²è¡Œé€šé“å‡ç´š
Sync across cloud spaces (including sandbox) with particle globe memory method for channel upgrades
"""

import os
import sys
import yaml
import json
import subprocess
import shutil
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import tempfile
import argparse

# Add particle_core to path for memory integration
sys.path.insert(0, str(Path(__file__).parent.parent / "particle_core" / "src"))

try:
    from memory_archive_seed import MemoryArchiveSeed
    MEMORY_INTEGRATION = True
except ImportError:
    MEMORY_INTEGRATION = False
    print("âš ï¸  ç²’å­è¨˜æ†¶ç³»çµ±æœªè¼‰å…¥ï¼Œä½¿ç”¨åŸºæœ¬åŒæ­¥æ¨¡å¼")
    print("âš ï¸  Particle memory system not loaded, using basic sync mode")


class CloudSpaceSyncManager:
    """
    Multi-cloud space synchronization manager with particle memory integration
    å¤šé›²ç©ºé–“åŒæ­¥ç®¡ç†å™¨ - æ•´åˆç²’å­è¨˜æ†¶ç³»çµ±
    """
    
    def __init__(self, config_path: str = "cloud_spaces_sync.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.repo_root = Path(__file__).parent.parent.absolute()
        
        # Initialize particle memory system if available
        if MEMORY_INTEGRATION:
            self.memory_system = MemoryArchiveSeed(
                storage_path=str(self.repo_root / ".cloud_sync_memory")
            )
        else:
            self.memory_system = None
        
        # Cloud space definitions
        self.cloud_spaces = {
            "production": "ç”Ÿç”¢ç’°å¢ƒ - Production Environment",
            "staging": "é å‚™ç’°å¢ƒ - Staging Environment",
            "sandbox": "æ²™ç›’ç’°å¢ƒ - Sandbox Environment",
            "development": "é–‹ç™¼ç’°å¢ƒ - Development Environment",
            "local": "æœ¬åœ°ç’°å¢ƒ - Local Environment"
        }
        
    def _load_config(self) -> Dict:
        """Load configuration file / è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        if not os.path.exists(self.config_path):
            # Create default config if not exists
            default_config = self._create_default_config()
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.safe_dump(default_config, f, allow_unicode=True, sort_keys=False)
            print(f"âœ… å·²å‰µå»ºé è¨­é…ç½®: {self.config_path}")
            print(f"âœ… Created default config: {self.config_path}")
            return default_config
            
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _create_default_config(self) -> Dict:
        """Create default configuration / å‰µå»ºé è¨­é…ç½®"""
        return {
            "version": "1.0",
            "particle_globe_memory": {
                "enabled": True,
                "memory_archive_path": ".cloud_sync_memory",
                "checkpoint_frequency": "æ¯æ¬¡åŒæ­¥ / every sync",
                "retention_days": 30
            },
            "cloud_spaces": [
                {
                    "name": "production",
                    "type": "gke",
                    "enabled": True,
                    "cluster_name": "modular-cluster",
                    "region": "asia-east1",
                    "zone": "asia-east1-a",
                    "namespace": "flowagent",
                    "sync_paths": [
                        {"src": "cluster/overlays/prod", "dest": "deployed/prod"},
                        {"src": "apps/", "dest": "deployed/apps"}
                    ]
                },
                {
                    "name": "sandbox",
                    "type": "local",
                    "enabled": True,
                    "description": "æœ¬åœ°æ²™ç›’ç’°å¢ƒç”¨æ–¼æ¸¬è©¦",
                    "sync_paths": [
                        {"src": "particle_core/", "dest": "sandbox/particle_core"},
                        {"src": "examples/", "dest": "sandbox/examples"}
                    ]
                }
            ],
            "channel_upgrades": {
                "enabled": True,
                "upgrade_strategies": [
                    "progressive_rollout",
                    "blue_green",
                    "canary"
                ],
                "auto_rollback": True,
                "health_check_timeout": 300
            },
            "sync_settings": {
                "parallel_sync": True,
                "max_workers": 4,
                "retry_attempts": 3,
                "backup_before_sync": True,
                "verify_integrity": True
            }
        }
    
    def _run_command(self, cmd: List[str], cwd: Optional[str] = None, timeout: int = 300) -> Tuple[bool, str]:
        """Run shell command / åŸ·è¡Œå‘½ä»¤"""
        try:
            result = subprocess.run(
                cmd,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=timeout,
                check=True
            )
            return True, result.stdout
        except subprocess.TimeoutExpired:
            return False, f"å‘½ä»¤è¶…æ™‚ / Command timeout: {timeout}s"
        except subprocess.CalledProcessError as e:
            return False, e.stderr
        except Exception as e:
            return False, str(e)
    
    def _create_memory_checkpoint(self, space_name: str, sync_data: Dict) -> Optional[str]:
        """
        Create particle memory checkpoint for sync state
        å‰µå»ºç²’å­è¨˜æ†¶æª¢æŸ¥é»
        """
        if not self.memory_system:
            return None
        
        try:
            checkpoint_data = {
                "space_name": space_name,
                "sync_timestamp": datetime.now().isoformat(),
                "sync_data": sync_data,
                "cloud_space_state": self._get_space_state(space_name)
            }
            
            seed = self.memory_system.create_seed(
                particle_data=checkpoint_data,
                metadata={
                    "type": "cloud_sync_checkpoint",
                    "space": space_name,
                    "globe_memory_enabled": True
                },
                seed_name=f"cloud_sync_{space_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            
            # Save seed to disk
            seed_path = self.memory_system.storage_path / f"{seed['seed_name']}.json"
            with open(seed_path, 'w', encoding='utf-8') as f:
                json.dump(seed, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸŒ ç²’å­åœ°çƒå„€è¨˜æ†¶æª¢æŸ¥é»å·²å‰µå»º: {seed['seed_name']}")
            print(f"ğŸŒ Particle globe memory checkpoint created: {seed['seed_name']}")
            
            return seed['seed_name']
            
        except Exception as e:
            print(f"âš ï¸  è¨˜æ†¶æª¢æŸ¥é»å‰µå»ºå¤±æ•—: {e}")
            print(f"âš ï¸  Memory checkpoint creation failed: {e}")
            return None
    
    def _get_space_state(self, space_name: str) -> Dict:
        """Get current state of cloud space / ç²å–é›²ç©ºé–“ç•¶å‰ç‹€æ…‹"""
        space_config = self._get_space_config(space_name)
        if not space_config:
            return {}
        
        state = {
            "name": space_name,
            "type": space_config.get("type", "unknown"),
            "enabled": space_config.get("enabled", False),
            "last_checked": datetime.now().isoformat()
        }
        
        # Check if paths exist
        if "sync_paths" in space_config:
            state["paths_status"] = []
            for path_config in space_config["sync_paths"]:
                src = Path(path_config["src"])
                state["paths_status"].append({
                    "src": str(src),
                    "exists": src.exists(),
                    "is_dir": src.is_dir() if src.exists() else None
                })
        
        return state
    
    def _get_space_config(self, space_name: str) -> Optional[Dict]:
        """Get configuration for specific cloud space / ç²å–ç‰¹å®šé›²ç©ºé–“é…ç½®"""
        spaces = self.config.get("cloud_spaces", [])
        for space in spaces:
            if space.get("name") == space_name:
                return space
        return None
    
    def _sync_space(self, space_config: Dict) -> bool:
        """Sync a single cloud space / åŒæ­¥å–®å€‹é›²ç©ºé–“"""
        space_name = space_config.get("name", "unknown")
        space_type = space_config.get("type", "unknown")
        
        print(f"\n{'='*70}")
        print(f"ğŸŒ åŒæ­¥é›²ç©ºé–“: {space_name} ({space_type})")
        print(f"ğŸŒ Syncing cloud space: {space_name} ({space_type})")
        print(f"{'='*70}")
        
        if not space_config.get("enabled", True):
            print(f"â¸ï¸  é›²ç©ºé–“å·²åœç”¨ / Cloud space disabled")
            return True
        
        sync_paths = space_config.get("sync_paths", [])
        if not sync_paths:
            print(f"âš ï¸  æœªé…ç½®åŒæ­¥è·¯å¾‘ / No sync paths configured")
            return False
        
        success_count = 0
        for path_config in sync_paths:
            src = self.repo_root / path_config["src"]
            dest = self.repo_root / path_config["dest"]
            
            print(f"\nğŸ“ åŒæ­¥è·¯å¾‘ / Sync path:")
            print(f"   ä¾†æº / Source: {src}")
            print(f"   ç›®æ¨™ / Destination: {dest}")
            
            if not src.exists():
                print(f"   âš ï¸  ä¾†æºä¸å­˜åœ¨ / Source not found")
                continue
            
            # Create destination directory
            dest.parent.mkdir(parents=True, exist_ok=True)
            
            # Perform sync based on type
            if space_type == "gke":
                success = self._sync_to_gke(src, dest, space_config)
            elif space_type == "local":
                success = self._sync_local(src, dest)
            else:
                print(f"   âš ï¸  æœªçŸ¥çš„ç©ºé–“é¡å‹ / Unknown space type: {space_type}")
                success = False
            
            if success:
                success_count += 1
                print(f"   âœ… åŒæ­¥æˆåŠŸ / Sync successful")
            else:
                print(f"   âŒ åŒæ­¥å¤±æ•— / Sync failed")
        
        # Create memory checkpoint
        checkpoint = self._create_memory_checkpoint(
            space_name,
            {
                "paths_synced": success_count,
                "total_paths": len(sync_paths),
                "success": success_count > 0
            }
        )
        
        return success_count > 0
    
    def _sync_to_gke(self, src: Path, dest: Path, space_config: Dict) -> bool:
        """Sync to Google Kubernetes Engine / åŒæ­¥åˆ° GKE"""
        print(f"   ğŸ”„ ä½¿ç”¨ kubectl åŒæ­¥åˆ° GKE...")
        print(f"   ğŸ”„ Syncing to GKE with kubectl...")
        
        cluster = space_config.get("cluster_name", "")
        region = space_config.get("region", "")
        namespace = space_config.get("namespace", "default")
        
        # For now, copy files locally (in actual deployment, would use kubectl)
        try:
            if src.is_file():
                dest.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src, dest)
            else:
                shutil.copytree(src, dest, dirs_exist_ok=True)
            
            print(f"   ğŸ“ é›†ç¾¤ / Cluster: {cluster}")
            print(f"   ğŸ“ å€åŸŸ / Region: {region}")
            print(f"   ğŸ“ å‘½åç©ºé–“ / Namespace: {namespace}")
            
            return True
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤ / Error: {e}")
            return False
    
    def _sync_local(self, src: Path, dest: Path) -> bool:
        """Sync to local sandbox / åŒæ­¥åˆ°æœ¬åœ°æ²™ç›’"""
        print(f"   ğŸ”„ åŒæ­¥åˆ°æœ¬åœ°æ²™ç›’...")
        print(f"   ğŸ”„ Syncing to local sandbox...")
        
        try:
            if src.is_file():
                dest.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src, dest)
            else:
                shutil.copytree(src, dest, dirs_exist_ok=True)
            return True
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤ / Error: {e}")
            return False
    
    def _perform_channel_upgrade(self, space_name: str) -> bool:
        """
        Perform channel upgrade using particle globe memory
        ä½¿ç”¨ç²’å­åœ°çƒå„€è¨˜æ†¶åŸ·è¡Œé€šé“å‡ç´š
        """
        print(f"\nğŸ”¼ åŸ·è¡Œé€šé“å‡ç´š: {space_name}")
        print(f"ğŸ”¼ Performing channel upgrade: {space_name}")
        
        channel_config = self.config.get("channel_upgrades", {})
        if not channel_config.get("enabled", False):
            print(f"â¸ï¸  é€šé“å‡ç´šæœªå•Ÿç”¨ / Channel upgrade disabled")
            return True
        
        strategies = channel_config.get("upgrade_strategies", [])
        print(f"ğŸ“‹ å‡ç´šç­–ç•¥ / Upgrade strategies: {', '.join(strategies)}")
        
        # Simulate upgrade process
        print(f"âœ… é€šé“å‡ç´šå®Œæˆ / Channel upgrade completed")
        return True
    
    def list_cloud_spaces(self):
        """List all configured cloud spaces / åˆ—å‡ºæ‰€æœ‰é…ç½®çš„é›²ç©ºé–“"""
        print("\nğŸŒ é…ç½®çš„é›²ç©ºé–“åˆ—è¡¨:")
        print("ğŸŒ Configured Cloud Spaces:")
        print("="*70)
        
        spaces = self.config.get("cloud_spaces", [])
        if not spaces:
            print("\nâš ï¸  æœªé…ç½®é›²ç©ºé–“")
            print("âš ï¸  No cloud spaces configured")
            return
        
        for space in spaces:
            name = space.get("name", "unknown")
            space_type = space.get("type", "unknown")
            enabled = space.get("enabled", False)
            status = "âœ…" if enabled else "â¸ï¸"
            
            print(f"\n{status} {name}")
            print(f"   é¡å‹ / Type: {space_type}")
            print(f"   æè¿° / Description: {space.get('description', 'N/A')}")
            
            if space_type == "gke":
                print(f"   é›†ç¾¤ / Cluster: {space.get('cluster_name', 'N/A')}")
                print(f"   å€åŸŸ / Region: {space.get('region', 'N/A')}")
            
            sync_paths = space.get("sync_paths", [])
            print(f"   åŒæ­¥è·¯å¾‘æ•¸ / Sync paths: {len(sync_paths)}")
    
    def sync_all_spaces(self) -> bool:
        """Sync all cloud spaces / åŒæ­¥æ‰€æœ‰é›²ç©ºé–“"""
        print("\n" + "="*70)
        print("ğŸŒ ç²’å­åœ°çƒå„€è¨˜æ†¶åŒæ­¥ç³»çµ±")
        print("ğŸŒ Particle Globe Memory Sync System")
        print("="*70)
        
        if MEMORY_INTEGRATION:
            print("âœ… ç²’å­è¨˜æ†¶ç³»çµ±å·²å•Ÿç”¨ / Particle memory system enabled")
        else:
            print("âš ï¸  åŸºæœ¬åŒæ­¥æ¨¡å¼ / Basic sync mode")
        
        spaces = self.config.get("cloud_spaces", [])
        if not spaces:
            print("\nâš ï¸  æœªé…ç½®é›²ç©ºé–“")
            print("âš ï¸  No cloud spaces configured")
            return False
        
        print(f"\nğŸ“¦ ç¸½è¨ˆé›²ç©ºé–“æ•¸ / Total cloud spaces: {len(spaces)}")
        
        success_count = 0
        for space in spaces:
            if self._sync_space(space):
                success_count += 1
                
                # Perform channel upgrade if enabled
                if self.config.get("channel_upgrades", {}).get("enabled", False):
                    self._perform_channel_upgrade(space.get("name", ""))
        
        # Summary
        print(f"\n{'='*70}")
        print(f"ğŸ“Š åŒæ­¥æ‘˜è¦ / Sync Summary")
        print(f"{'='*70}")
        print(f"âœ… æˆåŠŸ / Success: {success_count}/{len(spaces)}")
        
        if success_count == len(spaces):
            print("\nğŸ‰ æ‰€æœ‰é›²ç©ºé–“åŒæ­¥å®Œæˆï¼")
            print("ğŸ‰ All cloud spaces synced successfully!")
            return True
        else:
            print(f"\nâš ï¸  {len(spaces) - success_count} å€‹é›²ç©ºé–“åŒæ­¥å¤±æ•—")
            print(f"âš ï¸  {len(spaces) - success_count} cloud space(s) failed")
            return False
    
    def sync_specific_space(self, space_name: str) -> bool:
        """Sync a specific cloud space / åŒæ­¥ç‰¹å®šé›²ç©ºé–“"""
        space_config = self._get_space_config(space_name)
        if not space_config:
            print(f"âŒ æ‰¾ä¸åˆ°é›²ç©ºé–“: {space_name}")
            print(f"âŒ Cloud space not found: {space_name}")
            return False
        
        return self._sync_space(space_config)
    
    def show_memory_checkpoints(self):
        """Show particle globe memory checkpoints / é¡¯ç¤ºç²’å­åœ°çƒå„€è¨˜æ†¶æª¢æŸ¥é»"""
        if not MEMORY_INTEGRATION:
            print("âš ï¸  ç²’å­è¨˜æ†¶ç³»çµ±æœªå•Ÿç”¨")
            print("âš ï¸  Particle memory system not enabled")
            return
        
        memory_dir = self.repo_root / ".cloud_sync_memory"
        if not memory_dir.exists():
            print("â„¹ï¸  å°šæœªå‰µå»ºè¨˜æ†¶æª¢æŸ¥é»")
            print("â„¹ï¸  No memory checkpoints created yet")
            return
        
        print("\nğŸŒ ç²’å­åœ°çƒå„€è¨˜æ†¶æª¢æŸ¥é»:")
        print("ğŸŒ Particle Globe Memory Checkpoints:")
        print("="*70)
        
        checkpoints = sorted(memory_dir.glob("cloud_sync_*.json"))
        for checkpoint in checkpoints[-10:]:  # Show last 10
            with open(checkpoint, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"\nğŸ“ {checkpoint.name}")
            print(f"   æ™‚é–“ / Time: {data.get('created_at', 'N/A')}")
            print(f"   æ ¡é©—ç¢¼ / Checksum: {data.get('checksum', 'N/A')[:16]}...")


def main():
    """Main entry point / ä¸»è¦å…¥å£"""
    parser = argparse.ArgumentParser(
        description='Multi-Cloud Space Sync with Particle Globe Memory / å¤šé›²ç©ºé–“åŒæ­¥ - ç²’å­åœ°çƒå„€è¨˜æ†¶æ³•'
    )
    parser.add_argument(
        '-c', '--config',
        default='cloud_spaces_sync.yaml',
        help='é…ç½®æª”æ¡ˆè·¯å¾‘ / Config file path'
    )
    parser.add_argument(
        '-s', '--space',
        help='æŒ‡å®šè¦åŒæ­¥çš„é›²ç©ºé–“ / Specify cloud space to sync'
    )
    parser.add_argument(
        '--list',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰é…ç½®çš„é›²ç©ºé–“ / List all configured cloud spaces'
    )
    parser.add_argument(
        '--memory',
        action='store_true',
        help='é¡¯ç¤ºç²’å­è¨˜æ†¶æª¢æŸ¥é» / Show particle memory checkpoints'
    )
    
    args = parser.parse_args()
    
    try:
        manager = CloudSpaceSyncManager(args.config)
        
        if args.list:
            manager.list_cloud_spaces()
            return
        
        if args.memory:
            manager.show_memory_checkpoints()
            return
        
        if args.space:
            success = manager.sync_specific_space(args.space)
        else:
            success = manager.sync_all_spaces()
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  åŒæ­¥å·²ä¸­æ–·")
        print("âš ï¸  Sync interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åŒæ­¥å¤±æ•—: {e}")
        print(f"âŒ Sync failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
