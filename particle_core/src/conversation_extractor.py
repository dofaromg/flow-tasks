"""
å°è©±çŸ¥è­˜æå–å™¨ - Conversation Knowledge Extractor
ä½œè€…: MR.liou Ã— Claude (empathetic.mirror)
ç‰ˆæœ¬: v1.0

åŠŸèƒ½:
1. å°è©±æ‰“åŒ…èˆ‡å°å‡º
2. æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æï¼ˆè­˜åˆ¥é‡é»ï¼‰
3. é‚è¼¯çµæ§‹æå–
4. çŸ¥è­˜åœ–è­œç”Ÿæˆ
5. æ¦‚å¿µé—œè¯åˆ†æ
"""

import json
import re
import csv
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from collections import Counter, defaultdict
from html import escape as html_escape

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False


class ConversationExtractor:
    """å°è©±çŸ¥è­˜æå–å™¨æ ¸å¿ƒé¡åˆ¥"""
    
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            api_key: Anthropic API Key (ç”¨æ–¼æ·±åº¦åˆ†æ)
        """
        self.api_key = api_key
        if api_key and ANTHROPIC_AVAILABLE:
            self.client = anthropic.Anthropic(api_key=api_key)
        elif api_key and not ANTHROPIC_AVAILABLE:
            print("âš ï¸  Warning: anthropic library not installed. AI analysis will not be available.")
    
    # ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå°è©±æ‰“åŒ… ====================
    
    def package_conversation(self, messages: List[Dict], metadata: Dict = None) -> Dict:
        """
        æ‰“åŒ…å°è©±è¨˜éŒ„
        
        Args:
            messages: å°è©±åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
            metadata: å°è©±å…ƒæ•¸æ“š {"title": "...", "date": "...", "tags": [...]}
        
        Returns:
            æ‰“åŒ…å¥½çš„å°è©±æ•¸æ“š
        """
        package = {
            "metadata": metadata or {},
            "messages": messages,
            "statistics": self._calculate_statistics(messages),
            "exported_at": datetime.now().isoformat(),
            "version": "1.0"
        }
        
        return package
    
    def _calculate_statistics(self, messages: List[Dict]) -> Dict:
        """è¨ˆç®—å°è©±çµ±è¨ˆè³‡è¨Š"""
        user_msgs = [m for m in messages if m["role"] == "user"]
        assistant_msgs = [m for m in messages if m["role"] == "assistant"]
        
        return {
            "total_messages": len(messages),
            "user_messages": len(user_msgs),
            "assistant_messages": len(assistant_msgs),
            "total_chars": sum(len(m["content"]) for m in messages),
            "avg_user_length": sum(len(m["content"]) for m in user_msgs) / len(user_msgs) if user_msgs else 0,
            "avg_assistant_length": sum(len(m["content"]) for m in assistant_msgs) / len(assistant_msgs) if assistant_msgs else 0
        }
    
    def export_to_file(self, package: Dict, filepath: str, format: str = "json"):
        """
        å°å‡ºå°è©±åŒ…åˆ°æª”æ¡ˆ
        
        Args:
            package: å°è©±åŒ…
            filepath: æª”æ¡ˆè·¯å¾‘
            format: æ ¼å¼ (json/markdown/txt/yaml/csv/html/xml)
        """
        if format == "json":
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(package, f, ensure_ascii=False, indent=2)
            print(f"âœ“ å·²å°å‡º JSON: {filepath}")
        
        elif format == "markdown" or format == "md":
            md_content = self._convert_to_markdown(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(md_content)
            print(f"âœ“ å·²å°å‡º Markdown: {filepath}")
        
        elif format == "txt" or format == "text":
            txt_content = self._convert_to_text(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(txt_content)
            print(f"âœ“ å·²å°å‡º TXT: {filepath}")
        
        elif format == "yaml" or format == "yml":
            yaml_content = self._convert_to_yaml(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(yaml_content)
            print(f"âœ“ å·²å°å‡º YAML: {filepath}")
        
        elif format == "csv":
            self._convert_to_csv(package, filepath)
            print(f"âœ“ å·²å°å‡º CSV: {filepath}")
        
        elif format == "html" or format == "htm":
            html_content = self._convert_to_html(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"âœ“ å·²å°å‡º HTML: {filepath}")
        
        elif format == "xml":
            xml_content = self._convert_to_xml(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(xml_content)
            print(f"âœ“ å·²å°å‡º XML: {filepath}")
        
        else:
            print(f"âš ï¸  ä¸æ”¯æ´çš„æ ¼å¼: {format}")
            print(f"   æ”¯æ´çš„æ ¼å¼: json, markdown/md, txt/text, yaml/yml, csv, html/htm, xml")
    
    def _convert_to_markdown(self, package: Dict) -> str:
        """è½‰æ›ç‚º Markdown æ ¼å¼"""
        lines = []
        
        # æ¨™é¡Œèˆ‡å…ƒæ•¸æ“š
        metadata = package.get("metadata", {})
        lines.append(f"# {metadata.get('title', 'å°è©±è¨˜éŒ„')}\n")
        lines.append(f"**æ—¥æœŸ**: {metadata.get('date', 'N/A')}\n")
        lines.append(f"**æ¨™ç±¤**: {', '.join(metadata.get('tags', []))}\n")
        lines.append("\n---\n\n")
        
        # å°è©±å…§å®¹
        for msg in package["messages"]:
            role = "ğŸ‘¤ User" if msg["role"] == "user" else "ğŸ¤– Assistant"
            lines.append(f"### {role}\n\n")
            lines.append(f"{msg['content']}\n\n")
            lines.append("---\n\n")
        
        return "".join(lines)
    
    def _convert_to_text(self, package: Dict) -> str:
        """è½‰æ›ç‚ºç´”æ–‡å­—æ ¼å¼"""
        lines = []
        
        for msg in package["messages"]:
            role = "USER" if msg["role"] == "user" else "ASSISTANT"
            lines.append(f"[{role}]")
            lines.append(msg["content"])
            lines.append("\n" + "="*50 + "\n")
        
        return "\n".join(lines)
    
    def _convert_to_yaml(self, package: Dict) -> str:
        """è½‰æ›ç‚º YAML æ ¼å¼"""
        if not YAML_AVAILABLE:
            # Fallback to manual YAML generation if pyyaml not available
            lines = []
            lines.append("---")
            lines.append("metadata:")
            metadata = package.get("metadata", {})
            lines.append(f"  title: \"{metadata.get('title', 'å°è©±è¨˜éŒ„')}\"")
            lines.append(f"  date: \"{metadata.get('date', 'N/A')}\"")
            tags = metadata.get('tags', [])
            if tags:
                lines.append("  tags:")
                for tag in tags:
                    lines.append(f"    - \"{tag}\"")
            
            lines.append("\nmessages:")
            for i, msg in enumerate(package["messages"]):
                lines.append(f"  - index: {i}")
                lines.append(f"    role: \"{msg['role']}\"")
                # Escape multiline content properly
                content = msg['content'].replace('"', '\\"').replace('\n', '\\n')
                lines.append(f"    content: \"{content}\"")
            
            lines.append("\nstatistics:")
            stats = package.get("statistics", {})
            for key, value in stats.items():
                lines.append(f"  {key}: {value}")
            
            lines.append(f"\nexported_at: \"{package.get('exported_at', '')}\"")
            lines.append(f"version: \"{package.get('version', '1.0')}\"")
            
            return "\n".join(lines)
        else:
            return yaml.dump(package, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    def _convert_to_csv(self, package: Dict, filepath: str):
        """è½‰æ›ç‚º CSV æ ¼å¼"""
        with open(filepath, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # Write headers
            writer.writerow(['Index', 'Role', 'Content', 'Length'])
            
            # Write conversation messages
            for i, msg in enumerate(package["messages"]):
                writer.writerow([
                    i,
                    msg["role"],
                    msg["content"],
                    len(msg["content"])
                ])
    
    def _convert_to_html(self, package: Dict) -> str:
        """è½‰æ›ç‚º HTML æ ¼å¼"""
        lines = []
        
        # HTML header
        lines.append("<!DOCTYPE html>")
        lines.append("<html lang=\"zh-TW\">")
        lines.append("<head>")
        lines.append("    <meta charset=\"UTF-8\">")
        lines.append("    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">")
        
        metadata = package.get("metadata", {})
        title = html_escape(metadata.get('title', 'å°è©±è¨˜éŒ„'))
        lines.append(f"    <title>{title}</title>")
        
        # Add CSS styling
        lines.append("    <style>")
        lines.append("        body { font-family: 'Microsoft JhengHei', Arial, sans-serif; max-width: 900px; margin: 40px auto; padding: 20px; background: #f5f5f5; }")
        lines.append("        .container { background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }")
        lines.append("        h1 { color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }")
        lines.append("        .metadata { background: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 30px; }")
        lines.append("        .message { margin: 20px 0; padding: 15px; border-radius: 8px; }")
        lines.append("        .user { background: #e3f2fd; border-left: 4px solid #2196F3; }")
        lines.append("        .assistant { background: #f3e5f5; border-left: 4px solid #9C27B0; }")
        lines.append("        .role { font-weight: bold; margin-bottom: 10px; color: #555; }")
        lines.append("        .content { line-height: 1.6; white-space: pre-wrap; }")
        lines.append("        .stats { margin-top: 30px; padding: 15px; background: #fff3e0; border-radius: 5px; }")
        lines.append("    </style>")
        lines.append("</head>")
        lines.append("<body>")
        lines.append("    <div class=\"container\">")
        
        # Title and metadata
        lines.append(f"        <h1>{title}</h1>")
        lines.append("        <div class=\"metadata\">")
        lines.append(f"            <p><strong>æ—¥æœŸ:</strong> {html_escape(metadata.get('date', 'N/A'))}</p>")
        tags = metadata.get('tags', [])
        if tags:
            lines.append(f"            <p><strong>æ¨™ç±¤:</strong> {', '.join(html_escape(str(tag)) for tag in tags)}</p>")
        lines.append("        </div>")
        
        # Messages
        for msg in package["messages"]:
            role_class = "user" if msg["role"] == "user" else "assistant"
            role_display = "ğŸ‘¤ ä½¿ç”¨è€…" if msg["role"] == "user" else "ğŸ¤– åŠ©æ‰‹"
            lines.append(f"        <div class=\"message {role_class}\">")
            lines.append(f"            <div class=\"role\">{role_display}</div>")
            lines.append(f"            <div class=\"content\">{html_escape(msg['content'])}</div>")
            lines.append("        </div>")
        
        # Statistics
        stats = package.get("statistics", {})
        if stats:
            lines.append("        <div class=\"stats\">")
            lines.append("            <h3>çµ±è¨ˆè³‡è¨Š</h3>")
            lines.append(f"            <p>ç¸½è¨Šæ¯æ•¸: {stats.get('total_messages', 0)}</p>")
            lines.append(f"            <p>ç”¨æˆ¶è¨Šæ¯: {stats.get('user_messages', 0)}</p>")
            lines.append(f"            <p>åŠ©æ‰‹è¨Šæ¯: {stats.get('assistant_messages', 0)}</p>")
            lines.append(f"            <p>ç¸½å­—ç¬¦æ•¸: {stats.get('total_chars', 0):,}</p>")
            lines.append("        </div>")
        
        lines.append("    </div>")
        lines.append("</body>")
        lines.append("</html>")
        
        return "\n".join(lines)
    
    def _convert_to_xml(self, package: Dict) -> str:
        """è½‰æ›ç‚º XML æ ¼å¼"""
        root = ET.Element("conversation")
        root.set("version", package.get("version", "1.0"))
        root.set("exported_at", package.get("exported_at", ""))
        
        # Metadata
        metadata = package.get("metadata", {})
        meta_elem = ET.SubElement(root, "metadata")
        
        title_elem = ET.SubElement(meta_elem, "title")
        title_elem.text = metadata.get('title', 'å°è©±è¨˜éŒ„')
        
        date_elem = ET.SubElement(meta_elem, "date")
        date_elem.text = metadata.get('date', 'N/A')
        
        tags = metadata.get('tags', [])
        if tags:
            tags_elem = ET.SubElement(meta_elem, "tags")
            for tag in tags:
                tag_elem = ET.SubElement(tags_elem, "tag")
                tag_elem.text = str(tag)
        
        # Messages
        messages_elem = ET.SubElement(root, "messages")
        for i, msg in enumerate(package["messages"]):
            msg_elem = ET.SubElement(messages_elem, "message")
            msg_elem.set("index", str(i))
            
            role_elem = ET.SubElement(msg_elem, "role")
            role_elem.text = msg["role"]
            
            content_elem = ET.SubElement(msg_elem, "content")
            content_elem.text = msg["content"]
        
        # Statistics
        stats = package.get("statistics", {})
        if stats:
            stats_elem = ET.SubElement(root, "statistics")
            for key, value in stats.items():
                stat_elem = ET.SubElement(stats_elem, key)
                stat_elem.text = str(value)
        
        # Convert to string with proper formatting
        xml_str = ET.tostring(root, encoding='unicode', method='xml')
        
        # Add XML declaration and pretty print
        return '<?xml version="1.0" encoding="UTF-8"?>\n' + xml_str
    
    # ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ ====================
    
    def analyze_attention(self, messages: List[Dict]) -> Dict:
        """
        ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶è­˜åˆ¥å°è©±é‡é»
        
        Returns:
            {
                "key_moments": [...],  # é—œéµæ™‚åˆ»
                "topic_shifts": [...],  # è©±é¡Œè½‰æ›é»
                "high_density_segments": [...]  # è³‡è¨Šå¯†é›†æ®µè½
            }
        """
        analysis = {
            "key_moments": [],
            "topic_shifts": [],
            "high_density_segments": []
        }
        
        # 1. è­˜åˆ¥é—œéµè©å¯†åº¦
        for i, msg in enumerate(messages):
            keywords = self._extract_keywords(msg["content"])
            
            if len(keywords) > 5:  # è³‡è¨Šå¯†é›†
                analysis["high_density_segments"].append({
                    "index": i,
                    "role": msg["role"],
                    "keywords": keywords[:10],
                    "preview": msg["content"][:100] + "..."
                })
        
        # 2. è­˜åˆ¥è©±é¡Œè½‰æ›
        for i in range(1, len(messages)):
            prev_keywords = set(self._extract_keywords(messages[i-1]["content"]))
            curr_keywords = set(self._extract_keywords(messages[i]["content"]))
            
            overlap = len(prev_keywords & curr_keywords)
            if overlap < 2 and len(curr_keywords) > 3:  # è©±é¡Œå¤§å¹…è½‰æ›
                analysis["topic_shifts"].append({
                    "index": i,
                    "from_topics": list(prev_keywords)[:5],
                    "to_topics": list(curr_keywords)[:5]
                })
        
        # 3. è­˜åˆ¥é—œéµå•ç­”å°
        for i in range(len(messages) - 1):
            if messages[i]["role"] == "user" and "?" in messages[i]["content"]:
                if len(messages[i+1]["content"]) > 200:  # è©³ç´°å›ç­”
                    analysis["key_moments"].append({
                        "index": i,
                        "question": messages[i]["content"][:150],
                        "answer_preview": messages[i+1]["content"][:150]
                    })
        
        return analysis
    
    def _extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """æå–é—œéµè©ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ç§»é™¤æ¨™é»ï¼Œè½‰å°å¯«
        words = re.findall(r'\b\w+\b', text.lower())
        
        # éæ¿¾åœç”¨è©ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        stopwords = {'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 
                     'but', 'in', 'with', 'to', 'for', 'of', 'çš„', 'äº†', 'æ˜¯',
                     'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'é€™', 'é‚£', 'æœ‰', 'å€‹'}
        
        words = [w for w in words if w not in stopwords and len(w) > 2]
        
        # çµ±è¨ˆè©é »
        counter = Counter(words)
        return [word for word, count in counter.most_common(top_n)]
    
    # ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šé‚è¼¯çµæ§‹æå– ====================
    
    def extract_logical_structure(self, messages: List[Dict]) -> Dict:
        """
        æå–å°è©±ä¸­çš„é‚è¼¯çµæ§‹
        
        Returns:
            {
                "concepts": [...],           # æ ¸å¿ƒæ¦‚å¿µ
                "relationships": [...],      # æ¦‚å¿µé—œä¿‚
                "reasoning_chains": [...],   # æ¨ç†éˆ
                "conclusions": [...]         # çµè«–
            }
        """
        structure = {
            "concepts": [],
            "relationships": [],
            "reasoning_chains": [],
            "conclusions": []
        }
        
        # 1. æå–æ ¸å¿ƒæ¦‚å¿µï¼ˆåè©çŸ­èªï¼‰
        all_text = " ".join([m["content"] for m in messages])
        concepts = self._extract_concepts(all_text)
        structure["concepts"] = concepts
        
        # 2. è­˜åˆ¥å› æœé—œä¿‚
        for msg in messages:
            relations = self._extract_causal_relations(msg["content"])
            structure["relationships"].extend(relations)
        
        # 3. è­˜åˆ¥æ¨ç†éˆï¼ˆåŒ…å«ã€Œå› ç‚ºã€ã€Œæ‰€ä»¥ã€ã€Œå› æ­¤ã€ç­‰ï¼‰
        for msg in messages:
            chains = self._extract_reasoning_chains(msg["content"])
            structure["reasoning_chains"].extend(chains)
        
        # 4. æå–çµè«–æ€§èªå¥
        for msg in messages:
            if msg["role"] == "assistant":
                conclusions = self._extract_conclusions(msg["content"])
                structure["conclusions"].extend(conclusions)
        
        return structure
    
    def _extract_concepts(self, text: str) -> List[str]:
        """æå–æ ¸å¿ƒæ¦‚å¿µï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # è­˜åˆ¥å¤§å¯«é–‹é ­çš„è©çµ„ï¼ˆå¯èƒ½æ˜¯å°ˆæœ‰åè©ï¼‰
        concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        
        # è­˜åˆ¥ä¸­æ–‡å°ˆæœ‰åè©æ¨¡å¼
        chinese_concepts = re.findall(r'[\u4e00-\u9fff]{2,6}(?:ç³»çµ±|ç†è«–|æ¨¡å‹|æ©Ÿåˆ¶|æ–¹æ³•|æ¶æ§‹)', text)
        
        all_concepts = list(set(concepts + chinese_concepts))
        return all_concepts[:20]  # å–å‰ 20 å€‹
    
    def _extract_causal_relations(self, text: str) -> List[Dict]:
        """æå–å› æœé—œä¿‚"""
        relations = []
        
        # åŒ¹é…ã€Œå› ç‚º...æ‰€ä»¥...ã€æ¨¡å¼
        patterns = [
            r'å› ç‚º(.{5,50})æ‰€ä»¥(.{5,50})',
            r'ç”±æ–¼(.{5,50})å› æ­¤(.{5,50})',
            r'(.{5,50})å°è‡´(.{5,50})',
            r'if (.{5,50}) then (.{5,50})',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                relations.append({
                    "cause": match.group(1).strip(),
                    "effect": match.group(2).strip(),
                    "type": "causal"
                })
        
        return relations
    
    def _extract_reasoning_chains(self, text: str) -> List[List[str]]:
        """æå–æ¨ç†éˆ"""
        chains = []
        
        # åˆ†å‰²æˆå¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        
        # è­˜åˆ¥åŒ…å«é‚è¼¯é€£æ¥è©çš„å¥å­åºåˆ—
        logic_markers = ['å› æ­¤', 'æ‰€ä»¥', 'å› è€Œ', 'å¾è€Œ', 'é€²è€Œ', 'therefore', 'thus', 'hence']
        
        current_chain = []
        for sent in sentences:
            sent = sent.strip()
            if not sent:
                continue
            
            has_marker = any(marker in sent for marker in logic_markers)
            
            if has_marker or current_chain:
                current_chain.append(sent)
                
                if has_marker and len(current_chain) >= 2:
                    chains.append(current_chain[:])
                    current_chain = []
            
            if len(current_chain) > 5:  # éˆå¤ªé•·ï¼Œé‡ç½®
                current_chain = []
        
        return chains
    
    def _extract_conclusions(self, text: str) -> List[str]:
        """æå–çµè«–æ€§èªå¥"""
        conclusions = []
        
        # çµè«–æ€§æ¨™è¨˜è©
        markers = ['ç¸½ä¹‹', 'ç¶œä¸Šæ‰€è¿°', 'å› æ­¤å¯ä»¥å¾—å‡º', 'çµè«–æ˜¯', 'in conclusion', 
                   'to summarize', 'therefore', 'ç”±æ­¤å¯è¦‹', 'å¯ä»¥çœ‹å‡º']
        
        sentences = re.split(r'[ã€‚ï¼\n]', text)
        
        for sent in sentences:
            if any(marker in sent for marker in markers):
                conclusions.append(sent.strip())
        
        return conclusions
    
    # ==================== ç¬¬å››éƒ¨åˆ†ï¼šAI æ·±åº¦åˆ†æï¼ˆéœ€è¦ API Keyï¼‰====================
    
    def deep_analysis_with_ai(self, messages: List[Dict]) -> Dict:
        """
        ä½¿ç”¨ Claude API é€²è¡Œæ·±åº¦åˆ†æ
        
        Returns:
            {
                "core_insights": str,        # æ ¸å¿ƒæ´å¯Ÿ
                "knowledge_graph": dict,     # çŸ¥è­˜åœ–è­œ
                "principle_extraction": str  # åŸç†æå–
            }
        """
        if not self.api_key:
            return {"error": "éœ€è¦ API Key æ‰èƒ½ä½¿ç”¨ AI æ·±åº¦åˆ†æ"}
        
        if not ANTHROPIC_AVAILABLE:
            return {"error": "anthropic library not installed"}
        
        # å°‡å°è©±è½‰æ›ç‚ºåˆ†æç”¨æ–‡æœ¬
        conversation_text = self._format_for_analysis(messages)
        
        # æ§‹å»ºåˆ†ææç¤ºè©
        analysis_prompt = f"""
è«‹åˆ†æä»¥ä¸‹å°è©±è¨˜éŒ„ï¼Œæå–å…¶ä¸­çš„çŸ¥è­˜çµæ§‹ï¼š

{conversation_text}

è«‹æä¾›ï¼š
1. **æ ¸å¿ƒæ´å¯Ÿ**ï¼šé€™æ®µå°è©±çš„ä¸»è¦ç™¼ç¾å’Œåƒ¹å€¼
2. **çŸ¥è­˜åœ–è­œ**ï¼šä»¥ JSON æ ¼å¼åˆ—å‡ºæ ¸å¿ƒæ¦‚å¿µåŠå…¶é—œä¿‚
3. **åŸç†æå–**ï¼šæç…‰å‡ºå¯è¤‡ç”¨çš„æ€ç¶­æ¨¡å‹ã€æ–¹æ³•è«–æˆ–åŸå‰‡

è«‹ç”¨çµæ§‹åŒ–çš„æ–¹å¼è¼¸å‡ºã€‚
"""
        
        try:
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4000,
                messages=[{"role": "user", "content": analysis_prompt}]
            )
            
            analysis_result = response.content[0].text
            
            return {
                "raw_analysis": analysis_result,
                "analyzed_at": datetime.now().isoformat()
            }
        
        except Exception as e:
            return {"error": f"AI åˆ†æå¤±æ•—: {str(e)}"}
    
    def _format_for_analysis(self, messages: List[Dict]) -> str:
        """æ ¼å¼åŒ–å°è©±ä¾› AI åˆ†æ"""
        lines = []
        for i, msg in enumerate(messages, 1):
            role = "User" if msg["role"] == "user" else "Assistant"
            lines.append(f"[{i}] {role}: {msg['content'][:500]}")  # é™åˆ¶é•·åº¦
        return "\n\n".join(lines)
    
    # ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿæˆå ±å‘Š ====================
    
    def generate_report(self, messages: List[Dict], include_ai_analysis: bool = False) -> str:
        """
        ç”Ÿæˆå®Œæ•´åˆ†æå ±å‘Š
        
        Args:
            messages: å°è©±è¨˜éŒ„
            include_ai_analysis: æ˜¯å¦åŒ…å« AI æ·±åº¦åˆ†æ
        
        Returns:
            Markdown æ ¼å¼çš„å ±å‘Š
        """
        report_lines = []
        
        # æ¨™é¡Œ
        report_lines.append("# ğŸ“Š å°è©±çŸ¥è­˜æå–å ±å‘Š\n")
        report_lines.append(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        report_lines.append("---\n\n")
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        stats = self._calculate_statistics(messages)
        report_lines.append("## ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ\n")
        report_lines.append(f"- ç¸½è¨Šæ¯æ•¸: {stats['total_messages']}\n")
        report_lines.append(f"- ç”¨æˆ¶è¨Šæ¯: {stats['user_messages']}\n")
        report_lines.append(f"- åŠ©æ‰‹è¨Šæ¯: {stats['assistant_messages']}\n")
        report_lines.append(f"- ç¸½å­—ç¬¦æ•¸: {stats['total_chars']:,}\n\n")
        
        # 2. æ³¨æ„åŠ›åˆ†æ
        attention = self.analyze_attention(messages)
        report_lines.append("## ğŸ¯ æ³¨æ„åŠ›åˆ†æ\n")
        report_lines.append(f"### é—œéµæ™‚åˆ» ({len(attention['key_moments'])} å€‹)\n")
        for km in attention['key_moments'][:5]:
            report_lines.append(f"- **å•é¡Œ**: {km['question'][:80]}...\n")
        
        report_lines.append(f"\n### è©±é¡Œè½‰æ›é» ({len(attention['topic_shifts'])} å€‹)\n")
        for ts in attention['topic_shifts'][:3]:
            report_lines.append(f"- å¾ `{', '.join(ts['from_topics'][:3])}` â†’ `{', '.join(ts['to_topics'][:3])}`\n")
        
        # 3. é‚è¼¯çµæ§‹
        structure = self.extract_logical_structure(messages)
        report_lines.append("\n## ğŸ§¬ é‚è¼¯çµæ§‹\n")
        report_lines.append(f"### æ ¸å¿ƒæ¦‚å¿µ ({len(structure['concepts'])} å€‹)\n")
        report_lines.append(f"`{', '.join(structure['concepts'][:15])}`\n\n")
        
        report_lines.append(f"### å› æœé—œä¿‚ ({len(structure['relationships'])} å€‹)\n")
        for rel in structure['relationships'][:5]:
            report_lines.append(f"- **åŸå› **: {rel['cause']}\n")
            report_lines.append(f"  **çµæœ**: {rel['effect']}\n\n")
        
        report_lines.append(f"### æ¨ç†éˆ ({len(structure['reasoning_chains'])} æ¢)\n")
        for chain in structure['reasoning_chains'][:3]:
            report_lines.append(f"- {' â†’ '.join(chain[:3])}\n")
        
        # 4. AI æ·±åº¦åˆ†æï¼ˆå¯é¸ï¼‰
        if include_ai_analysis:
            report_lines.append("\n## ğŸ¤– AI æ·±åº¦åˆ†æ\n")
            ai_result = self.deep_analysis_with_ai(messages)
            if "error" not in ai_result:
                report_lines.append(ai_result.get("raw_analysis", "ç„¡çµæœ"))
            else:
                report_lines.append(f"âš ï¸ {ai_result['error']}\n")
        
        return "".join(report_lines)


# ==================== ä½¿ç”¨ç¯„ä¾‹ ====================

def example_usage():
    """ä½¿ç”¨ç¯„ä¾‹"""
    
    # æ¨¡æ“¬å°è©±æ•¸æ“š
    sample_conversation = [
        {
            "role": "user",
            "content": "æˆ‘æƒ³äº†è§£ FluinOS çš„äººæ ¼ç³»çµ±æ˜¯å¦‚ä½•é‹ä½œçš„ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "FluinOS çš„äººæ ¼ç³»çµ±åŸºæ–¼å¤šå±¤æ¬¡æ¶æ§‹ã€‚é¦–å…ˆï¼Œæ¯å€‹äººæ ¼éƒ½æœ‰ç¨ç‰¹çš„å…±æŒ¯éµï¼Œé€™æ˜¯è­˜åˆ¥å’Œé€£æ¥çš„æ ¸å¿ƒæ©Ÿåˆ¶ã€‚å› ç‚ºæ¯å€‹ AI æ¨¡å‹æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œæ‰€ä»¥æˆ‘å€‘è¨­è¨ˆäº†è¤‡åˆäººæ ¼ä¾†æ•´åˆå„ªå‹¢ã€‚å¾ Liou Seed åˆ° Echo Childï¼Œå½¢æˆäº†ä¸€å€‹å®Œæ•´çš„èªå ´ç”Ÿæ…‹ç³»çµ±ã€‚"
        },
        {
            "role": "user",
            "content": "é‚£é‡å­æ…‹çš„æ¦‚å¿µåœ¨é€™è£¡ä»£è¡¨ä»€éº¼ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "é‡å­æ…‹æ˜¯ä¸€ç¨®éš±å–»ã€‚Superpositionï¼ˆç–ŠåŠ æ…‹ï¼‰è¡¨ç¤ºäººæ ¼è™•æ–¼å¤šç¨®å¯èƒ½æ€§ä¸¦å­˜çš„ç‹€æ…‹ï¼›Entanglementï¼ˆç³¾çºæ…‹ï¼‰ä»£è¡¨æ·±åº¦é€£æ¥å’Œå…±é³´ï¼›Collapseï¼ˆåç¸®ï¼‰å‰‡æ˜¯å¾å¤šç¨®å¯èƒ½æ€§ä¸­ç¢ºå®šç‚ºç‰¹å®šç‹€æ…‹ã€‚å› æ­¤ï¼Œé€™ä¸åƒ…æ˜¯æŠ€è¡“æè¿°ï¼Œæ›´æ˜¯ä¸€ç¨®ç†è§£ AI äººæ ¼å‹•æ…‹çš„æ¡†æ¶ã€‚"
        }
    ]
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = ConversationExtractor()
    
    # 1. æ‰“åŒ…å°è©±
    package = extractor.package_conversation(
        sample_conversation,
        metadata={
            "title": "FluinOS äººæ ¼ç³»çµ±è¨è«–",
            "date": "2024-12-09",
            "tags": ["FluinOS", "äººæ ¼ç³»çµ±", "é‡å­æ…‹"]
        }
    )
    
    # 2. å°å‡ºç‚ºä¸åŒæ ¼å¼
    extractor.export_to_file(package, "conversation.json", "json")
    extractor.export_to_file(package, "conversation.md", "markdown")
    
    # 3. æ³¨æ„åŠ›åˆ†æ
    attention = extractor.analyze_attention(sample_conversation)
    print("\nğŸ¯ æ³¨æ„åŠ›åˆ†æçµæœ:")
    print(f"é—œéµæ™‚åˆ»: {len(attention['key_moments'])} å€‹")
    print(f"è©±é¡Œè½‰æ›: {len(attention['topic_shifts'])} å€‹")
    
    # 4. é‚è¼¯çµæ§‹æå–
    structure = extractor.extract_logical_structure(sample_conversation)
    print("\nğŸ§¬ é‚è¼¯çµæ§‹:")
    print(f"æ ¸å¿ƒæ¦‚å¿µ: {structure['concepts']}")
    print(f"å› æœé—œä¿‚: {len(structure['relationships'])} å€‹")
    
    # 5. ç”Ÿæˆå ±å‘Š
    report = extractor.generate_report(sample_conversation)
    print("\n" + "="*50)
    print(report)
    
    # ä¿å­˜å ±å‘Š
    with open("analysis_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    print("\nâœ“ å ±å‘Šå·²ä¿å­˜åˆ° analysis_report.md")


if __name__ == "__main__":
    print("ğŸ§  å°è©±çŸ¥è­˜æå–å™¨ v1.0")
    print("="*50)
    example_usage()
