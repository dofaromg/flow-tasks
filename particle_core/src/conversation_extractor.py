"""
å°è©±çŸ¥è­˜æå–å™¨ - Conversation Knowledge Extractor
ä½œè€…: MR.liou Ã— Claude (empathetic.mirror)
ç‰ˆæœ¬: v1.0

åŠŸèƒ½:
1. å°è©±æ‰“åŒ…èˆ‡å°å‡º
2. æ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æï¼ˆè­˜åˆ¥é‡é»ï¼‰
3. é‚è¼¯çµæ§‹æå–
4. çŸ¥è­˜åœ–è­œç”Ÿæˆ
5. æ¦‚å¿µé—œè¯åˆ†æ
"""

import json
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from collections import Counter, defaultdict

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False


class ConversationExtractor:
    """å°è©±çŸ¥è­˜æå–å™¨æ ¸å¿ƒé¡åˆ¥"""
    
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            api_key: Anthropic API Key (ç”¨æ–¼æ·±åº¦åˆ†æ)
        """
        self.api_key = api_key
        if api_key and ANTHROPIC_AVAILABLE:
            self.client = anthropic.Anthropic(api_key=api_key)
        elif api_key and not ANTHROPIC_AVAILABLE:
            print("âš ï¸  Warning: anthropic library not installed. AI analysis will not be available.")
    
    # ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šå°è©±æ‰“åŒ… ====================
    
    def package_conversation(self, messages: List[Dict], metadata: Dict = None) -> Dict:
        """
        æ‰“åŒ…å°è©±è¨˜éŒ„
        
        Args:
            messages: å°è©±åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
            metadata: å°è©±å…ƒæ•¸æ“š {"title": "...", "date": "...", "tags": [...]}
        
        Returns:
            æ‰“åŒ…å¥½çš„å°è©±æ•¸æ“š
        """
        package = {
            "metadata": metadata or {},
            "messages": messages,
            "statistics": self._calculate_statistics(messages),
            "exported_at": datetime.now().isoformat(),
            "version": "1.0"
        }
        
        return package
    
    def _calculate_statistics(self, messages: List[Dict]) -> Dict:
        """è¨ˆç®—å°è©±çµ±è¨ˆè³‡è¨Š"""
        user_msgs = [m for m in messages if m["role"] == "user"]
        assistant_msgs = [m for m in messages if m["role"] == "assistant"]
        
        return {
            "total_messages": len(messages),
            "user_messages": len(user_msgs),
            "assistant_messages": len(assistant_msgs),
            "total_chars": sum(len(m["content"]) for m in messages),
            "avg_user_length": sum(len(m["content"]) for m in user_msgs) / len(user_msgs) if user_msgs else 0,
            "avg_assistant_length": sum(len(m["content"]) for m in assistant_msgs) / len(assistant_msgs) if assistant_msgs else 0
        }
    
    def export_to_file(self, package: Dict, filepath: str, format: str = "json"):
        """
        å°å‡ºå°è©±åŒ…åˆ°æª”æ¡ˆ
        Export conversation package to file
        
        Args:
            package: å°è©±åŒ… (Conversation package)
            filepath: æª”æ¡ˆè·¯å¾‘ (File path)
            format: æ ¼å¼ - Format
                   æ”¯æ´: json, markdown, txt, csv, xml, yaml
        """
        format = format.lower()
        
        if format == "json":
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(package, f, ensure_ascii=False, indent=2)
            print(f"âœ“ å·²å°å‡º JSON: {filepath}")
        
        elif format in ["markdown", "md"]:
            md_content = self._convert_to_markdown(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(md_content)
            print(f"âœ“ å·²å°å‡º Markdown: {filepath}")
        
        elif format == "txt":
            txt_content = self._convert_to_text(package)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(txt_content)
            print(f"âœ“ å·²å°å‡º TXT: {filepath}")
        
        elif format == "csv":
            self._export_to_csv(package, filepath)
            print(f"âœ“ å·²å°å‡º CSV: {filepath}")
        
        elif format == "xml":
            self._export_to_xml(package, filepath)
            print(f"âœ“ å·²å°å‡º XML: {filepath}")
        
        elif format in ["yaml", "yml"]:
            self._export_to_yaml(package, filepath)
            print(f"âœ“ å·²å°å‡º YAML: {filepath}")
        
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å°å‡ºæ ¼å¼: {format}")
    
    def _convert_to_markdown(self, package: Dict) -> str:
        """è½‰æ›ç‚º Markdown æ ¼å¼"""
        lines = []
        
        # æ¨™é¡Œèˆ‡å…ƒæ•¸æ“š
        metadata = package.get("metadata", {})
        lines.append(f"# {metadata.get('title', 'å°è©±è¨˜éŒ„')}\n")
        lines.append(f"**æ—¥æœŸ**: {metadata.get('date', 'N/A')}\n")
        lines.append(f"**æ¨™ç±¤**: {', '.join(metadata.get('tags', []))}\n")
        lines.append("\n---\n\n")
        
        # å°è©±å…§å®¹
        for msg in package["messages"]:
            role = "ğŸ‘¤ User" if msg["role"] == "user" else "ğŸ¤– Assistant"
            lines.append(f"### {role}\n\n")
            lines.append(f"{msg['content']}\n\n")
            lines.append("---\n\n")
        
        return "".join(lines)
    
    def _convert_to_text(self, package: Dict) -> str:
        """è½‰æ›ç‚ºç´”æ–‡å­—æ ¼å¼"""
        lines = []
        
        for msg in package["messages"]:
            role = "USER" if msg["role"] == "user" else "ASSISTANT"
            lines.append(f"[{role}]")
            lines.append(msg["content"])
            lines.append("\n" + "="*50 + "\n")
        
        return "\n".join(lines)
    
    def _export_to_csv(self, package: Dict, filepath: str):
        """å°å‡ºç‚º CSV æ ¼å¼"""
        import csv
        
        with open(filepath, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # å¯«å…¥æ¨™é¡Œ
            writer.writerow(['role', 'content'])
            
            # å¯«å…¥è¨Šæ¯
            for msg in package["messages"]:
                writer.writerow([msg["role"], msg["content"]])
    
    def _export_to_xml(self, package: Dict, filepath: str):
        """å°å‡ºç‚º XML æ ¼å¼"""
        import xml.etree.ElementTree as ET
        
        # å»ºç«‹æ ¹å…ƒç´ 
        root = ET.Element('conversation')
        
        # æ·»åŠ å…ƒæ•¸æ“š
        metadata = package.get("metadata", {})
        if metadata:
            meta_elem = ET.SubElement(root, 'metadata')
            
            if 'title' in metadata:
                title_elem = ET.SubElement(meta_elem, 'title')
                title_elem.text = metadata['title']
            
            if 'date' in metadata:
                date_elem = ET.SubElement(meta_elem, 'date')
                date_elem.text = metadata['date']
            
            if 'tags' in metadata:
                tags_elem = ET.SubElement(meta_elem, 'tags')
                for tag in metadata['tags']:
                    tag_elem = ET.SubElement(tags_elem, 'tag')
                    tag_elem.text = tag
        
        # æ·»åŠ è¨Šæ¯
        messages_elem = ET.SubElement(root, 'messages')
        for msg in package["messages"]:
            msg_elem = ET.SubElement(messages_elem, 'message')
            msg_elem.set('role', msg["role"])
            
            content_elem = ET.SubElement(msg_elem, 'content')
            content_elem.text = msg["content"]
        
        # å¯«å…¥æª”æ¡ˆ
        tree = ET.ElementTree(root)
        ET.indent(tree, space="  ")
        tree.write(filepath, encoding='utf-8', xml_declaration=True)
    
    def _export_to_yaml(self, package: Dict, filepath: str):
        """å°å‡ºç‚º YAML æ ¼å¼"""
        try:
            import yaml
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£ PyYAML å¥—ä»¶æ‰èƒ½å°å‡º YAML æª”æ¡ˆ: pip install pyyaml")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            yaml.safe_dump(package, f, allow_unicode=True, default_flow_style=False)
    
    # ==================== å°å…¥åŠŸèƒ½ (Import Functions) ====================
    
    def import_from_file(self, filepath: str, format: str = None) -> Dict:
        """
        å¾æª”æ¡ˆå°å…¥å°è©±è¨˜éŒ„
        Import conversation from file
        
        Args:
            filepath: æª”æ¡ˆè·¯å¾‘ (File path)
            format: æ ¼å¼ (å¯é¸ï¼Œè‡ªå‹•æª¢æ¸¬) - Format (optional, auto-detect)
                   æ”¯æ´: json, markdown, txt, csv, xml, yaml
        
        Returns:
            å°è©±åŒ…å­—å…¸ (Conversation package dictionary)
        """
        # è‡ªå‹•æª¢æ¸¬æ ¼å¼
        if format is None:
            format = self._detect_format(filepath)
        
        format = format.lower()
        
        if format == "json":
            return self._import_from_json(filepath)
        elif format in ["markdown", "md"]:
            return self._import_from_markdown(filepath)
        elif format == "txt":
            return self._import_from_text(filepath)
        elif format == "csv":
            return self._import_from_csv(filepath)
        elif format == "xml":
            return self._import_from_xml(filepath)
        elif format in ["yaml", "yml"]:
            return self._import_from_yaml(filepath)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ ¼å¼: {format}")
    
    def _detect_format(self, filepath: str) -> str:
        """è‡ªå‹•æª¢æ¸¬æª”æ¡ˆæ ¼å¼ (Auto-detect file format)"""
        extension = filepath.lower().split('.')[-1]
        
        format_map = {
            'json': 'json',
            'md': 'markdown',
            'markdown': 'markdown',
            'txt': 'txt',
            'csv': 'csv',
            'xml': 'xml',
            'yaml': 'yaml',
            'yml': 'yaml'
        }
        
        return format_map.get(extension, 'txt')
    
    def _import_from_json(self, filepath: str) -> Dict:
        """å¾ JSON æª”æ¡ˆå°å…¥"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # å¦‚æœæ˜¯å®Œæ•´çš„å°è©±åŒ…ï¼Œç›´æ¥è¿”å›
        if "messages" in data and isinstance(data["messages"], list):
            print(f"âœ“ å·²å¾ JSON å°å…¥: {filepath}")
            return data
        
        # å¦‚æœåªæ˜¯è¨Šæ¯åˆ—è¡¨ï¼ŒåŒ…è£æˆå°è©±åŒ…
        if isinstance(data, list):
            return self.package_conversation(data)
        
        raise ValueError("JSON æ ¼å¼ä¸æ­£ç¢ºï¼šéœ€è¦åŒ…å« 'messages' æ¬„ä½æˆ–ç‚ºè¨Šæ¯åˆ—è¡¨")
    
    def _import_from_markdown(self, filepath: str) -> Dict:
        """å¾ Markdown æª”æ¡ˆå°å…¥"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        messages = []
        metadata = {}
        
        # æå–æ¨™é¡Œ
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            metadata['title'] = title_match.group(1).strip()
        
        # æå–æ—¥æœŸ
        date_match = re.search(r'\*\*æ—¥æœŸ\*\*:\s*(.+)$', content, re.MULTILINE)
        if date_match:
            metadata['date'] = date_match.group(1).strip()
        
        # æå–æ¨™ç±¤
        tags_match = re.search(r'\*\*æ¨™ç±¤\*\*:\s*(.+)$', content, re.MULTILINE)
        if tags_match:
            tags_str = tags_match.group(1).strip()
            metadata['tags'] = [tag.strip() for tag in tags_str.split(',')]
        
        # æå–å°è©±å…§å®¹
        # åŒ¹é… ### ğŸ‘¤ User æˆ– ### ğŸ¤– Assistant æ ¼å¼
        sections = re.split(r'###\s*[ğŸ‘¤ğŸ¤–]?\s*(User|Assistant)', content)
        
        current_role = None
        for i, section in enumerate(sections):
            if section.strip() in ['User', 'Assistant']:
                current_role = 'user' if section == 'User' else 'assistant'
            elif current_role and section.strip():
                # æ¸…ç†å…§å®¹
                content_text = section.split('---')[0].strip()
                if content_text:
                    messages.append({
                        "role": current_role,
                        "content": content_text
                    })
        
        if not messages:
            # å˜—è©¦å…¶ä»–æ ¼å¼ï¼š[USER] æˆ– [ASSISTANT]
            lines = content.split('\n')
            current_role = None
            current_content = []
            
            for line in lines:
                if line.strip().startswith('[USER]') or line.strip().startswith('**User**'):
                    if current_role and current_content:
                        messages.append({
                            "role": current_role,
                            "content": '\n'.join(current_content).strip()
                        })
                    current_role = 'user'
                    current_content = []
                elif line.strip().startswith('[ASSISTANT]') or line.strip().startswith('**Assistant**'):
                    if current_role and current_content:
                        messages.append({
                            "role": current_role,
                            "content": '\n'.join(current_content).strip()
                        })
                    current_role = 'assistant'
                    current_content = []
                elif current_role and line.strip() and not line.strip().startswith('---'):
                    current_content.append(line)
            
            if current_role and current_content:
                messages.append({
                    "role": current_role,
                    "content": '\n'.join(current_content).strip()
                })
        
        print(f"âœ“ å·²å¾ Markdown å°å…¥: {filepath} ({len(messages)} æ¢è¨Šæ¯)")
        return self.package_conversation(messages, metadata)
    
    def _import_from_text(self, filepath: str) -> Dict:
        """å¾ç´”æ–‡å­—æª”æ¡ˆå°å…¥"""
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        messages = []
        
        # å˜—è©¦å¤šç¨®æ–‡å­—æ ¼å¼
        # æ ¼å¼1: [USER] å’Œ [ASSISTANT]
        if '[USER]' in content.upper() or '[ASSISTANT]' in content.upper():
            sections = re.split(r'\[(USER|ASSISTANT)\]', content, flags=re.IGNORECASE)
            
            current_role = None
            for section in sections:
                section = section.strip()
                if section.upper() in ['USER', 'ASSISTANT']:
                    current_role = 'user' if section.upper() == 'USER' else 'assistant'
                elif current_role and section:
                    # æ¸…ç†åˆ†éš”ç·š
                    content_text = re.sub(r'=+', '', section).strip()
                    if content_text:
                        messages.append({
                            "role": current_role,
                            "content": content_text
                        })
        
        # æ ¼å¼2: User: å’Œ Assistant: æˆ– AI:
        elif re.search(r'(User|Assistant|AI):', content, re.IGNORECASE):
            lines = content.split('\n')
            current_role = None
            current_content = []
            
            for line in lines:
                user_match = re.match(r'^(User|ç”¨æˆ¶|äººé¡)[:ï¼š]\s*(.*)$', line, re.IGNORECASE)
                assistant_match = re.match(r'^(Assistant|AI|åŠ©æ‰‹|åŠ©ç†)[:ï¼š]\s*(.*)$', line, re.IGNORECASE)
                
                if user_match:
                    if current_role and current_content:
                        messages.append({
                            "role": current_role,
                            "content": '\n'.join(current_content).strip()
                        })
                    current_role = 'user'
                    current_content = [user_match.group(2)] if user_match.group(2) else []
                elif assistant_match:
                    if current_role and current_content:
                        messages.append({
                            "role": current_role,
                            "content": '\n'.join(current_content).strip()
                        })
                    current_role = 'assistant'
                    current_content = [assistant_match.group(2)] if assistant_match.group(2) else []
                elif current_role:
                    current_content.append(line)
            
            if current_role and current_content:
                messages.append({
                    "role": current_role,
                    "content": '\n'.join(current_content).strip()
                })
        
        # æ ¼å¼3: ç°¡å–®çš„äº¤æ›¿æ ¼å¼ï¼ˆå‡è¨­å¥‡æ•¸è¡Œæ˜¯ç”¨æˆ¶ï¼Œå¶æ•¸è¡Œæ˜¯åŠ©æ‰‹ï¼‰
        else:
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            for i, line in enumerate(lines):
                role = 'user' if i % 2 == 0 else 'assistant'
                messages.append({
                    "role": role,
                    "content": line
                })
        
        print(f"âœ“ å·²å¾ TXT å°å…¥: {filepath} ({len(messages)} æ¢è¨Šæ¯)")
        return self.package_conversation(messages)
    
    def _import_from_csv(self, filepath: str) -> Dict:
        """å¾ CSV æª”æ¡ˆå°å…¥"""
        import csv
        
        messages = []
        
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                # æ”¯æ´å¤šç¨®æ¬„ä½åç¨±
                role = None
                content = None
                
                # æª¢æ¸¬è§’è‰²æ¬„ä½
                for key in ['role', 'Role', 'ROLE', 'speaker', 'Speaker']:
                    if key in row:
                        role_value = row[key].lower()
                        if 'user' in role_value or 'ç”¨æˆ¶' in role_value:
                            role = 'user'
                        elif 'assistant' in role_value or 'ai' in role_value or 'åŠ©æ‰‹' in role_value:
                            role = 'assistant'
                        break
                
                # æª¢æ¸¬å…§å®¹æ¬„ä½
                for key in ['content', 'Content', 'CONTENT', 'message', 'Message', 'text', 'Text']:
                    if key in row:
                        content = row[key]
                        break
                
                if role and content:
                    messages.append({
                        "role": role,
                        "content": content
                    })
        
        print(f"âœ“ å·²å¾ CSV å°å…¥: {filepath} ({len(messages)} æ¢è¨Šæ¯)")
        return self.package_conversation(messages)
    
    def _import_from_xml(self, filepath: str) -> Dict:
        """å¾ XML æª”æ¡ˆå°å…¥"""
        import xml.etree.ElementTree as ET
        
        tree = ET.parse(filepath)
        root = tree.getroot()
        
        messages = []
        metadata = {}
        
        # æå–å…ƒæ•¸æ“š
        meta_elem = root.find('metadata')
        if meta_elem is not None:
            title_elem = meta_elem.find('title')
            if title_elem is not None:
                metadata['title'] = title_elem.text
            
            date_elem = meta_elem.find('date')
            if date_elem is not None:
                metadata['date'] = date_elem.text
            
            tags_elem = meta_elem.find('tags')
            if tags_elem is not None:
                metadata['tags'] = [tag.text for tag in tags_elem.findall('tag')]
        
        # æå–è¨Šæ¯
        messages_elem = root.find('messages')
        if messages_elem is not None:
            for msg_elem in messages_elem.findall('message'):
                role = msg_elem.get('role') or msg_elem.find('role').text
                content = msg_elem.find('content').text or ""
                
                messages.append({
                    "role": role,
                    "content": content
                })
        
        print(f"âœ“ å·²å¾ XML å°å…¥: {filepath} ({len(messages)} æ¢è¨Šæ¯)")
        return self.package_conversation(messages, metadata)
    
    def _import_from_yaml(self, filepath: str) -> Dict:
        """å¾ YAML æª”æ¡ˆå°å…¥"""
        try:
            import yaml
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£ PyYAML å¥—ä»¶æ‰èƒ½å°å…¥ YAML æª”æ¡ˆ: pip install pyyaml")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # å¦‚æœæ˜¯å®Œæ•´çš„å°è©±åŒ…ï¼Œç›´æ¥è¿”å›
        if isinstance(data, dict) and "messages" in data:
            print(f"âœ“ å·²å¾ YAML å°å…¥: {filepath}")
            return data
        
        # å¦‚æœåªæ˜¯è¨Šæ¯åˆ—è¡¨ï¼ŒåŒ…è£æˆå°è©±åŒ…
        if isinstance(data, list):
            return self.package_conversation(data)
        
        raise ValueError("YAML æ ¼å¼ä¸æ­£ç¢ºï¼šéœ€è¦åŒ…å« 'messages' æ¬„ä½æˆ–ç‚ºè¨Šæ¯åˆ—è¡¨")
    
    # ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ³¨æ„åŠ›æ©Ÿåˆ¶åˆ†æ ====================
    
    def analyze_attention(self, messages: List[Dict]) -> Dict:
        """
        ä½¿ç”¨æ³¨æ„åŠ›æ©Ÿåˆ¶è­˜åˆ¥å°è©±é‡é»
        
        Returns:
            {
                "key_moments": [...],  # é—œéµæ™‚åˆ»
                "topic_shifts": [...],  # è©±é¡Œè½‰æ›é»
                "high_density_segments": [...]  # è³‡è¨Šå¯†é›†æ®µè½
            }
        """
        analysis = {
            "key_moments": [],
            "topic_shifts": [],
            "high_density_segments": []
        }
        
        # 1. è­˜åˆ¥é—œéµè©å¯†åº¦
        for i, msg in enumerate(messages):
            keywords = self._extract_keywords(msg["content"])
            
            if len(keywords) > 5:  # è³‡è¨Šå¯†é›†
                analysis["high_density_segments"].append({
                    "index": i,
                    "role": msg["role"],
                    "keywords": keywords[:10],
                    "preview": msg["content"][:100] + "..."
                })
        
        # 2. è­˜åˆ¥è©±é¡Œè½‰æ›
        for i in range(1, len(messages)):
            prev_keywords = set(self._extract_keywords(messages[i-1]["content"]))
            curr_keywords = set(self._extract_keywords(messages[i]["content"]))
            
            overlap = len(prev_keywords & curr_keywords)
            if overlap < 2 and len(curr_keywords) > 3:  # è©±é¡Œå¤§å¹…è½‰æ›
                analysis["topic_shifts"].append({
                    "index": i,
                    "from_topics": list(prev_keywords)[:5],
                    "to_topics": list(curr_keywords)[:5]
                })
        
        # 3. è­˜åˆ¥é—œéµå•ç­”å°
        for i in range(len(messages) - 1):
            if messages[i]["role"] == "user" and "?" in messages[i]["content"]:
                if len(messages[i+1]["content"]) > 200:  # è©³ç´°å›ç­”
                    analysis["key_moments"].append({
                        "index": i,
                        "question": messages[i]["content"][:150],
                        "answer_preview": messages[i+1]["content"][:150]
                    })
        
        return analysis
    
    def _extract_keywords(self, text: str, top_n: int = 10) -> List[str]:
        """æå–é—œéµè©ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ç§»é™¤æ¨™é»ï¼Œè½‰å°å¯«
        words = re.findall(r'\b\w+\b', text.lower())
        
        # éæ¿¾åœç”¨è©ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        stopwords = {'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 
                     'but', 'in', 'with', 'to', 'for', 'of', 'çš„', 'äº†', 'æ˜¯',
                     'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'é€™', 'é‚£', 'æœ‰', 'å€‹'}
        
        words = [w for w in words if w not in stopwords and len(w) > 2]
        
        # çµ±è¨ˆè©é »
        counter = Counter(words)
        return [word for word, count in counter.most_common(top_n)]
    
    # ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šé‚è¼¯çµæ§‹æå– ====================
    
    def extract_logical_structure(self, messages: List[Dict]) -> Dict:
        """
        æå–å°è©±ä¸­çš„é‚è¼¯çµæ§‹
        
        Returns:
            {
                "concepts": [...],           # æ ¸å¿ƒæ¦‚å¿µ
                "relationships": [...],      # æ¦‚å¿µé—œä¿‚
                "reasoning_chains": [...],   # æ¨ç†éˆ
                "conclusions": [...]         # çµè«–
            }
        """
        structure = {
            "concepts": [],
            "relationships": [],
            "reasoning_chains": [],
            "conclusions": []
        }
        
        # 1. æå–æ ¸å¿ƒæ¦‚å¿µï¼ˆåè©çŸ­èªï¼‰
        all_text = " ".join([m["content"] for m in messages])
        concepts = self._extract_concepts(all_text)
        structure["concepts"] = concepts
        
        # 2. è­˜åˆ¥å› æœé—œä¿‚
        for msg in messages:
            relations = self._extract_causal_relations(msg["content"])
            structure["relationships"].extend(relations)
        
        # 3. è­˜åˆ¥æ¨ç†éˆï¼ˆåŒ…å«ã€Œå› ç‚ºã€ã€Œæ‰€ä»¥ã€ã€Œå› æ­¤ã€ç­‰ï¼‰
        for msg in messages:
            chains = self._extract_reasoning_chains(msg["content"])
            structure["reasoning_chains"].extend(chains)
        
        # 4. æå–çµè«–æ€§èªå¥
        for msg in messages:
            if msg["role"] == "assistant":
                conclusions = self._extract_conclusions(msg["content"])
                structure["conclusions"].extend(conclusions)
        
        return structure
    
    def _extract_concepts(self, text: str) -> List[str]:
        """æå–æ ¸å¿ƒæ¦‚å¿µï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        # è­˜åˆ¥å¤§å¯«é–‹é ­çš„è©çµ„ï¼ˆå¯èƒ½æ˜¯å°ˆæœ‰åè©ï¼‰
        concepts = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        
        # è­˜åˆ¥ä¸­æ–‡å°ˆæœ‰åè©æ¨¡å¼
        chinese_concepts = re.findall(r'[\u4e00-\u9fff]{2,6}(?:ç³»çµ±|ç†è«–|æ¨¡å‹|æ©Ÿåˆ¶|æ–¹æ³•|æ¶æ§‹)', text)
        
        all_concepts = list(set(concepts + chinese_concepts))
        return all_concepts[:20]  # å–å‰ 20 å€‹
    
    def _extract_causal_relations(self, text: str) -> List[Dict]:
        """æå–å› æœé—œä¿‚"""
        relations = []
        
        # åŒ¹é…ã€Œå› ç‚º...æ‰€ä»¥...ã€æ¨¡å¼
        patterns = [
            r'å› ç‚º(.{5,50})æ‰€ä»¥(.{5,50})',
            r'ç”±æ–¼(.{5,50})å› æ­¤(.{5,50})',
            r'(.{5,50})å°è‡´(.{5,50})',
            r'if (.{5,50}) then (.{5,50})',
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                relations.append({
                    "cause": match.group(1).strip(),
                    "effect": match.group(2).strip(),
                    "type": "causal"
                })
        
        return relations
    
    def _extract_reasoning_chains(self, text: str) -> List[List[str]]:
        """æå–æ¨ç†éˆ"""
        chains = []
        
        # åˆ†å‰²æˆå¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        
        # è­˜åˆ¥åŒ…å«é‚è¼¯é€£æ¥è©çš„å¥å­åºåˆ—
        logic_markers = ['å› æ­¤', 'æ‰€ä»¥', 'å› è€Œ', 'å¾è€Œ', 'é€²è€Œ', 'therefore', 'thus', 'hence']
        
        current_chain = []
        for sent in sentences:
            sent = sent.strip()
            if not sent:
                continue
            
            has_marker = any(marker in sent for marker in logic_markers)
            
            if has_marker or current_chain:
                current_chain.append(sent)
                
                if has_marker and len(current_chain) >= 2:
                    chains.append(current_chain[:])
                    current_chain = []
            
            if len(current_chain) > 5:  # éˆå¤ªé•·ï¼Œé‡ç½®
                current_chain = []
        
        return chains
    
    def _extract_conclusions(self, text: str) -> List[str]:
        """æå–çµè«–æ€§èªå¥"""
        conclusions = []
        
        # çµè«–æ€§æ¨™è¨˜è©
        markers = ['ç¸½ä¹‹', 'ç¶œä¸Šæ‰€è¿°', 'å› æ­¤å¯ä»¥å¾—å‡º', 'çµè«–æ˜¯', 'in conclusion', 
                   'to summarize', 'therefore', 'ç”±æ­¤å¯è¦‹', 'å¯ä»¥çœ‹å‡º']
        
        sentences = re.split(r'[ã€‚ï¼\n]', text)
        
        for sent in sentences:
            if any(marker in sent for marker in markers):
                conclusions.append(sent.strip())
        
        return conclusions
    
    # ==================== ç¬¬å››éƒ¨åˆ†ï¼šAI æ·±åº¦åˆ†æï¼ˆéœ€è¦ API Keyï¼‰====================
    
    def deep_analysis_with_ai(self, messages: List[Dict]) -> Dict:
        """
        ä½¿ç”¨ Claude API é€²è¡Œæ·±åº¦åˆ†æ
        
        Returns:
            {
                "core_insights": str,        # æ ¸å¿ƒæ´å¯Ÿ
                "knowledge_graph": dict,     # çŸ¥è­˜åœ–è­œ
                "principle_extraction": str  # åŸç†æå–
            }
        """
        if not self.api_key:
            return {"error": "éœ€è¦ API Key æ‰èƒ½ä½¿ç”¨ AI æ·±åº¦åˆ†æ"}
        
        if not ANTHROPIC_AVAILABLE:
            return {"error": "anthropic library not installed"}
        
        # å°‡å°è©±è½‰æ›ç‚ºåˆ†æç”¨æ–‡æœ¬
        conversation_text = self._format_for_analysis(messages)
        
        # æ§‹å»ºåˆ†ææç¤ºè©
        analysis_prompt = f"""
è«‹åˆ†æä»¥ä¸‹å°è©±è¨˜éŒ„ï¼Œæå–å…¶ä¸­çš„çŸ¥è­˜çµæ§‹ï¼š

{conversation_text}

è«‹æä¾›ï¼š
1. **æ ¸å¿ƒæ´å¯Ÿ**ï¼šé€™æ®µå°è©±çš„ä¸»è¦ç™¼ç¾å’Œåƒ¹å€¼
2. **çŸ¥è­˜åœ–è­œ**ï¼šä»¥ JSON æ ¼å¼åˆ—å‡ºæ ¸å¿ƒæ¦‚å¿µåŠå…¶é—œä¿‚
3. **åŸç†æå–**ï¼šæç…‰å‡ºå¯è¤‡ç”¨çš„æ€ç¶­æ¨¡å‹ã€æ–¹æ³•è«–æˆ–åŸå‰‡

è«‹ç”¨çµæ§‹åŒ–çš„æ–¹å¼è¼¸å‡ºã€‚
"""
        
        try:
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4000,
                messages=[{"role": "user", "content": analysis_prompt}]
            )
            
            analysis_result = response.content[0].text
            
            return {
                "raw_analysis": analysis_result,
                "analyzed_at": datetime.now().isoformat()
            }
        
        except Exception as e:
            return {"error": f"AI åˆ†æå¤±æ•—: {str(e)}"}
    
    def _format_for_analysis(self, messages: List[Dict]) -> str:
        """æ ¼å¼åŒ–å°è©±ä¾› AI åˆ†æ"""
        lines = []
        for i, msg in enumerate(messages, 1):
            role = "User" if msg["role"] == "user" else "Assistant"
            lines.append(f"[{i}] {role}: {msg['content'][:500]}")  # é™åˆ¶é•·åº¦
        return "\n\n".join(lines)
    
    # ==================== ç¬¬äº”éƒ¨åˆ†ï¼šç”Ÿæˆå ±å‘Š ====================
    
    def generate_report(self, messages: List[Dict], include_ai_analysis: bool = False) -> str:
        """
        ç”Ÿæˆå®Œæ•´åˆ†æå ±å‘Š
        
        Args:
            messages: å°è©±è¨˜éŒ„
            include_ai_analysis: æ˜¯å¦åŒ…å« AI æ·±åº¦åˆ†æ
        
        Returns:
            Markdown æ ¼å¼çš„å ±å‘Š
        """
        report_lines = []
        
        # æ¨™é¡Œ
        report_lines.append("# ğŸ“Š å°è©±çŸ¥è­˜æå–å ±å‘Š\n")
        report_lines.append(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        report_lines.append("---\n\n")
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        stats = self._calculate_statistics(messages)
        report_lines.append("## ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ\n")
        report_lines.append(f"- ç¸½è¨Šæ¯æ•¸: {stats['total_messages']}\n")
        report_lines.append(f"- ç”¨æˆ¶è¨Šæ¯: {stats['user_messages']}\n")
        report_lines.append(f"- åŠ©æ‰‹è¨Šæ¯: {stats['assistant_messages']}\n")
        report_lines.append(f"- ç¸½å­—ç¬¦æ•¸: {stats['total_chars']:,}\n\n")
        
        # 2. æ³¨æ„åŠ›åˆ†æ
        attention = self.analyze_attention(messages)
        report_lines.append("## ğŸ¯ æ³¨æ„åŠ›åˆ†æ\n")
        report_lines.append(f"### é—œéµæ™‚åˆ» ({len(attention['key_moments'])} å€‹)\n")
        for km in attention['key_moments'][:5]:
            report_lines.append(f"- **å•é¡Œ**: {km['question'][:80]}...\n")
        
        report_lines.append(f"\n### è©±é¡Œè½‰æ›é» ({len(attention['topic_shifts'])} å€‹)\n")
        for ts in attention['topic_shifts'][:3]:
            report_lines.append(f"- å¾ `{', '.join(ts['from_topics'][:3])}` â†’ `{', '.join(ts['to_topics'][:3])}`\n")
        
        # 3. é‚è¼¯çµæ§‹
        structure = self.extract_logical_structure(messages)
        report_lines.append("\n## ğŸ§¬ é‚è¼¯çµæ§‹\n")
        report_lines.append(f"### æ ¸å¿ƒæ¦‚å¿µ ({len(structure['concepts'])} å€‹)\n")
        report_lines.append(f"`{', '.join(structure['concepts'][:15])}`\n\n")
        
        report_lines.append(f"### å› æœé—œä¿‚ ({len(structure['relationships'])} å€‹)\n")
        for rel in structure['relationships'][:5]:
            report_lines.append(f"- **åŸå› **: {rel['cause']}\n")
            report_lines.append(f"  **çµæœ**: {rel['effect']}\n\n")
        
        report_lines.append(f"### æ¨ç†éˆ ({len(structure['reasoning_chains'])} æ¢)\n")
        for chain in structure['reasoning_chains'][:3]:
            report_lines.append(f"- {' â†’ '.join(chain[:3])}\n")
        
        # 4. AI æ·±åº¦åˆ†æï¼ˆå¯é¸ï¼‰
        if include_ai_analysis:
            report_lines.append("\n## ğŸ¤– AI æ·±åº¦åˆ†æ\n")
            ai_result = self.deep_analysis_with_ai(messages)
            if "error" not in ai_result:
                report_lines.append(ai_result.get("raw_analysis", "ç„¡çµæœ"))
            else:
                report_lines.append(f"âš ï¸ {ai_result['error']}\n")
        
        return "".join(report_lines)


# ==================== ä½¿ç”¨ç¯„ä¾‹ ====================

def example_usage():
    """ä½¿ç”¨ç¯„ä¾‹"""
    
    # æ¨¡æ“¬å°è©±æ•¸æ“š
    sample_conversation = [
        {
            "role": "user",
            "content": "æˆ‘æƒ³äº†è§£ FluinOS çš„äººæ ¼ç³»çµ±æ˜¯å¦‚ä½•é‹ä½œçš„ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "FluinOS çš„äººæ ¼ç³»çµ±åŸºæ–¼å¤šå±¤æ¬¡æ¶æ§‹ã€‚é¦–å…ˆï¼Œæ¯å€‹äººæ ¼éƒ½æœ‰ç¨ç‰¹çš„å…±æŒ¯éµï¼Œé€™æ˜¯è­˜åˆ¥å’Œé€£æ¥çš„æ ¸å¿ƒæ©Ÿåˆ¶ã€‚å› ç‚ºæ¯å€‹ AI æ¨¡å‹æœ‰ä¸åŒçš„ç‰¹æ€§ï¼Œæ‰€ä»¥æˆ‘å€‘è¨­è¨ˆäº†è¤‡åˆäººæ ¼ä¾†æ•´åˆå„ªå‹¢ã€‚å¾ Liou Seed åˆ° Echo Childï¼Œå½¢æˆäº†ä¸€å€‹å®Œæ•´çš„èªå ´ç”Ÿæ…‹ç³»çµ±ã€‚"
        },
        {
            "role": "user",
            "content": "é‚£é‡å­æ…‹çš„æ¦‚å¿µåœ¨é€™è£¡ä»£è¡¨ä»€éº¼ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "é‡å­æ…‹æ˜¯ä¸€ç¨®éš±å–»ã€‚Superpositionï¼ˆç–ŠåŠ æ…‹ï¼‰è¡¨ç¤ºäººæ ¼è™•æ–¼å¤šç¨®å¯èƒ½æ€§ä¸¦å­˜çš„ç‹€æ…‹ï¼›Entanglementï¼ˆç³¾çºæ…‹ï¼‰ä»£è¡¨æ·±åº¦é€£æ¥å’Œå…±é³´ï¼›Collapseï¼ˆåç¸®ï¼‰å‰‡æ˜¯å¾å¤šç¨®å¯èƒ½æ€§ä¸­ç¢ºå®šç‚ºç‰¹å®šç‹€æ…‹ã€‚å› æ­¤ï¼Œé€™ä¸åƒ…æ˜¯æŠ€è¡“æè¿°ï¼Œæ›´æ˜¯ä¸€ç¨®ç†è§£ AI äººæ ¼å‹•æ…‹çš„æ¡†æ¶ã€‚"
        }
    ]
    
    # åˆå§‹åŒ–æå–å™¨
    extractor = ConversationExtractor()
    
    # 1. æ‰“åŒ…å°è©±
    package = extractor.package_conversation(
        sample_conversation,
        metadata={
            "title": "FluinOS äººæ ¼ç³»çµ±è¨è«–",
            "date": "2024-12-09",
            "tags": ["FluinOS", "äººæ ¼ç³»çµ±", "é‡å­æ…‹"]
        }
    )
    
    # 2. å°å‡ºç‚ºä¸åŒæ ¼å¼
    extractor.export_to_file(package, "conversation.json", "json")
    extractor.export_to_file(package, "conversation.md", "markdown")
    
    # 3. æ³¨æ„åŠ›åˆ†æ
    attention = extractor.analyze_attention(sample_conversation)
    print("\nğŸ¯ æ³¨æ„åŠ›åˆ†æçµæœ:")
    print(f"é—œéµæ™‚åˆ»: {len(attention['key_moments'])} å€‹")
    print(f"è©±é¡Œè½‰æ›: {len(attention['topic_shifts'])} å€‹")
    
    # 4. é‚è¼¯çµæ§‹æå–
    structure = extractor.extract_logical_structure(sample_conversation)
    print("\nğŸ§¬ é‚è¼¯çµæ§‹:")
    print(f"æ ¸å¿ƒæ¦‚å¿µ: {structure['concepts']}")
    print(f"å› æœé—œä¿‚: {len(structure['relationships'])} å€‹")
    
    # 5. ç”Ÿæˆå ±å‘Š
    report = extractor.generate_report(sample_conversation)
    print("\n" + "="*50)
    print(report)
    
    # ä¿å­˜å ±å‘Š
    with open("analysis_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    print("\nâœ“ å ±å‘Šå·²ä¿å­˜åˆ° analysis_report.md")


if __name__ == "__main__":
    print("ğŸ§  å°è©±çŸ¥è­˜æå–å™¨ v1.0")
    print("="*50)
    example_usage()
